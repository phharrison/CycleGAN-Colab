{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CycleGAN",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5VIGyIus8Vr7"
      },
      "source": [
        "Take a look at the [repository](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix) for more information"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7wNjDKdQy35h"
      },
      "source": [
        "# Install"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TRm-USlsHgEV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "efa783c6-72a0-48fa-9837-97e56f232f4d"
      },
      "source": [
        "!git clone https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'pytorch-CycleGAN-and-pix2pix'...\n",
            "remote: Enumerating objects: 2337, done.\u001b[K\n",
            "remote: Total 2337 (delta 0), reused 0 (delta 0), pack-reused 2337\u001b[K\n",
            "Receiving objects: 100% (2337/2337), 8.09 MiB | 36.03 MiB/s, done.\n",
            "Resolving deltas: 100% (1499/1499), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pt3igws3eiVp"
      },
      "source": [
        "import os\n",
        "os.chdir('pytorch-CycleGAN-and-pix2pix/')"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z1EySlOXwwoa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e7b40b0c-8f92-42ae-81ae-3b3d815f6887"
      },
      "source": [
        "!pip install -r requirements.txt"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch>=1.4.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 1)) (1.7.0+cu101)\n",
            "Requirement already satisfied: torchvision>=0.5.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 2)) (0.8.1+cu101)\n",
            "Collecting dominate>=2.4.0\n",
            "  Downloading https://files.pythonhosted.org/packages/ef/a8/4354f8122c39e35516a2708746d89db5e339c867abbd8e0179bccee4b7f9/dominate-2.6.0-py2.py3-none-any.whl\n",
            "Collecting visdom>=0.1.8.8\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c9/75/e078f5a2e1df7e0d3044749089fc2823e62d029cc027ed8ae5d71fafcbdc/visdom-0.1.8.9.tar.gz (676kB)\n",
            "\u001b[K     |████████████████████████████████| 686kB 18.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.6/dist-packages (from torch>=1.4.0->-r requirements.txt (line 1)) (3.7.4.3)\n",
            "Requirement already satisfied: dataclasses in /usr/local/lib/python3.6/dist-packages (from torch>=1.4.0->-r requirements.txt (line 1)) (0.8)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch>=1.4.0->-r requirements.txt (line 1)) (1.18.5)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch>=1.4.0->-r requirements.txt (line 1)) (0.16.0)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision>=0.5.0->-r requirements.txt (line 2)) (7.0.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from visdom>=0.1.8.8->-r requirements.txt (line 4)) (1.4.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from visdom>=0.1.8.8->-r requirements.txt (line 4)) (2.23.0)\n",
            "Requirement already satisfied: tornado in /usr/local/lib/python3.6/dist-packages (from visdom>=0.1.8.8->-r requirements.txt (line 4)) (5.1.1)\n",
            "Requirement already satisfied: pyzmq in /usr/local/lib/python3.6/dist-packages (from visdom>=0.1.8.8->-r requirements.txt (line 4)) (20.0.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from visdom>=0.1.8.8->-r requirements.txt (line 4)) (1.15.0)\n",
            "Collecting jsonpatch\n",
            "  Downloading https://files.pythonhosted.org/packages/af/b3/715016f25700433b2aa017f1a525cccbf052c9385bd7b652d0c083e2fba6/jsonpatch-1.27-py2.py3-none-any.whl\n",
            "Collecting torchfile\n",
            "  Downloading https://files.pythonhosted.org/packages/91/af/5b305f86f2d218091af657ddb53f984ecbd9518ca9fe8ef4103a007252c9/torchfile-0.1.0.tar.gz\n",
            "Collecting websocket-client\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/4c/5f/f61b420143ed1c8dc69f9eaec5ff1ac36109d52c80de49d66e0c36c3dfdf/websocket_client-0.57.0-py2.py3-none-any.whl (200kB)\n",
            "\u001b[K     |████████████████████████████████| 204kB 46.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->visdom>=0.1.8.8->-r requirements.txt (line 4)) (2020.11.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->visdom>=0.1.8.8->-r requirements.txt (line 4)) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->visdom>=0.1.8.8->-r requirements.txt (line 4)) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->visdom>=0.1.8.8->-r requirements.txt (line 4)) (1.24.3)\n",
            "Collecting jsonpointer>=1.9\n",
            "  Downloading https://files.pythonhosted.org/packages/18/b0/a80d29577c08eea401659254dfaed87f1af45272899e1812d7e01b679bc5/jsonpointer-2.0-py2.py3-none-any.whl\n",
            "Building wheels for collected packages: visdom, torchfile\n",
            "  Building wheel for visdom (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for visdom: filename=visdom-0.1.8.9-cp36-none-any.whl size=655251 sha256=9451ce88880098d6b00f81c5c5ce6d5b7fbf0115f81ee64687ec3a19cb62841f\n",
            "  Stored in directory: /root/.cache/pip/wheels/70/19/a7/6d589ed967f4dfefd33bc166d081257bd4ed0cb618dccfd62a\n",
            "  Building wheel for torchfile (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for torchfile: filename=torchfile-0.1.0-cp36-none-any.whl size=5711 sha256=c870bd94ba2ef74e31596f08017468e1548a711a2e3b03b6364fe07980ea2abb\n",
            "  Stored in directory: /root/.cache/pip/wheels/b1/c3/d6/9a1cc8f3a99a0fc1124cae20153f36af59a6e683daca0a0814\n",
            "Successfully built visdom torchfile\n",
            "Installing collected packages: dominate, jsonpointer, jsonpatch, torchfile, websocket-client, visdom\n",
            "Successfully installed dominate-2.6.0 jsonpatch-1.27 jsonpointer-2.0 torchfile-0.1.0 visdom-0.1.8.9 websocket-client-0.57.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8daqlgVhw29P"
      },
      "source": [
        "# Datasets\n",
        "\n",
        "Download one of the official datasets with:\n",
        "\n",
        "-   `bash ./datasets/download_cyclegan_dataset.sh [apple2orange, orange2apple, summer2winter_yosemite, winter2summer_yosemite, horse2zebra, zebra2horse, monet2photo, style_monet, style_cezanne, style_ukiyoe, style_vangogh, sat2map, map2sat, cityscapes_photo2label, cityscapes_label2photo, facades_photo2label, facades_label2photo, iphone2dslr_flower]`\n",
        "\n",
        "Or use your own dataset by creating the appropriate folders and adding in the images.\n",
        "\n",
        "-   Create a dataset folder under `/dataset` for your dataset.\n",
        "-   Create subfolders `testA`, `testB`, `trainA`, and `trainB` under your dataset's folder. Place any images you want to transform from a to b (cat2dog) in the `testA` folder, images you want to transform from b to a (dog2cat) in the `testB` folder, and do the same for the `trainA` and `trainB` folders."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gYVYlbfdZvYa"
      },
      "source": [
        "**PHH**: *If creating your own dataset, populate the* `testA`, `testB`, `trainA` *and* `trainB` *directories within a folder of your choosing, then zip that folder and upload to the* `pytorch-CycleGAN-and-pix2pix/datasets` *directory.* *This next command will unzip the directory.*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vrdOettJxaCc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "42a62b76-2e24-497d-88ea-64944ed82158"
      },
      "source": [
        "!unzip ./datasets/rendering.zip -d /datasets/"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  ./datasets/rendering.zip\n",
            "   creating: rendering/\n",
            "   creating: rendering/testA/\n",
            "  inflating: rendering/testA/Animation000.jpg  \n",
            "  inflating: rendering/testA/Animation001.jpg  \n",
            "  inflating: rendering/testA/Animation002.jpg  \n",
            "  inflating: rendering/testA/Animation003.jpg  \n",
            "  inflating: rendering/testA/Animation004.jpg  \n",
            "  inflating: rendering/testA/Animation005.jpg  \n",
            "  inflating: rendering/testA/Animation006.jpg  \n",
            "  inflating: rendering/testA/Animation007.jpg  \n",
            "  inflating: rendering/testA/Animation008.jpg  \n",
            "  inflating: rendering/testA/Animation009.jpg  \n",
            "   creating: rendering/trainA/\n",
            "  inflating: rendering/trainA/Animation000.jpg  \n",
            "  inflating: rendering/trainA/Animation001.jpg  \n",
            "  inflating: rendering/trainA/Animation002.jpg  \n",
            "  inflating: rendering/trainA/Animation003.jpg  \n",
            "  inflating: rendering/trainA/Animation004.jpg  \n",
            "  inflating: rendering/trainA/Animation005.jpg  \n",
            "  inflating: rendering/trainA/Animation006.jpg  \n",
            "  inflating: rendering/trainA/Animation007.jpg  \n",
            "  inflating: rendering/trainA/Animation008.jpg  \n",
            "  inflating: rendering/trainA/Animation009.jpg  \n",
            "  inflating: rendering/trainA/Animation010.jpg  \n",
            "  inflating: rendering/trainA/Animation011.jpg  \n",
            "  inflating: rendering/trainA/Animation012.jpg  \n",
            "  inflating: rendering/trainA/Animation013.jpg  \n",
            "  inflating: rendering/trainA/Animation014.jpg  \n",
            "  inflating: rendering/trainA/Animation015.jpg  \n",
            "  inflating: rendering/trainA/Animation016.jpg  \n",
            "  inflating: rendering/trainA/Animation017.jpg  \n",
            "  inflating: rendering/trainA/Animation018.jpg  \n",
            "  inflating: rendering/trainA/Animation019.jpg  \n",
            "  inflating: rendering/trainA/Animation020.jpg  \n",
            "  inflating: rendering/trainA/Animation021.jpg  \n",
            "  inflating: rendering/trainA/Animation022.jpg  \n",
            "  inflating: rendering/trainA/Animation023.jpg  \n",
            "  inflating: rendering/trainA/Animation024.jpg  \n",
            "  inflating: rendering/trainA/Animation025.jpg  \n",
            "  inflating: rendering/trainA/Animation026.jpg  \n",
            "  inflating: rendering/trainA/Animation027.jpg  \n",
            "  inflating: rendering/trainA/Animation028.jpg  \n",
            "  inflating: rendering/trainA/Animation029.jpg  \n",
            "  inflating: rendering/trainA/Animation030.jpg  \n",
            "  inflating: rendering/trainA/Animation031.jpg  \n",
            "  inflating: rendering/trainA/Animation032.jpg  \n",
            "  inflating: rendering/trainA/Animation033.jpg  \n",
            "  inflating: rendering/trainA/Animation034.jpg  \n",
            "  inflating: rendering/trainA/Animation035.jpg  \n",
            "  inflating: rendering/trainA/Animation036.jpg  \n",
            "  inflating: rendering/trainA/Animation037.jpg  \n",
            "  inflating: rendering/trainA/Animation038.jpg  \n",
            "  inflating: rendering/trainA/Animation039.jpg  \n",
            "  inflating: rendering/trainA/Animation040.jpg  \n",
            "  inflating: rendering/trainA/Animation041.jpg  \n",
            "  inflating: rendering/trainA/Animation042.jpg  \n",
            "  inflating: rendering/trainA/Animation043.jpg  \n",
            "  inflating: rendering/trainA/Animation044.jpg  \n",
            "  inflating: rendering/trainA/Animation045.jpg  \n",
            "  inflating: rendering/trainA/Animation046.jpg  \n",
            "  inflating: rendering/trainA/Animation047.jpg  \n",
            "  inflating: rendering/trainA/Animation048.jpg  \n",
            "  inflating: rendering/trainA/Animation049.jpg  \n",
            "  inflating: rendering/trainA/Animation050.jpg  \n",
            "  inflating: rendering/trainA/Animation051.jpg  \n",
            "  inflating: rendering/trainA/Animation052.jpg  \n",
            "  inflating: rendering/trainA/Animation053.jpg  \n",
            "  inflating: rendering/trainA/Animation054.jpg  \n",
            "  inflating: rendering/trainA/Animation055.jpg  \n",
            "  inflating: rendering/trainA/Animation056.jpg  \n",
            "  inflating: rendering/trainA/Animation057.jpg  \n",
            "  inflating: rendering/trainA/Animation058.jpg  \n",
            "  inflating: rendering/trainA/Animation059.jpg  \n",
            "  inflating: rendering/trainA/Animation060.jpg  \n",
            "  inflating: rendering/trainA/Animation061.jpg  \n",
            "  inflating: rendering/trainA/Animation062.jpg  \n",
            "  inflating: rendering/trainA/Animation063.jpg  \n",
            "  inflating: rendering/trainA/Animation064.jpg  \n",
            "  inflating: rendering/trainA/Animation065.jpg  \n",
            "  inflating: rendering/trainA/Animation066.jpg  \n",
            "  inflating: rendering/trainA/Animation067.jpg  \n",
            "  inflating: rendering/trainA/Animation068.jpg  \n",
            "  inflating: rendering/trainA/Animation069.jpg  \n",
            "  inflating: rendering/trainA/Animation070.jpg  \n",
            "  inflating: rendering/trainA/Animation071.jpg  \n",
            "  inflating: rendering/trainA/Animation072.jpg  \n",
            "  inflating: rendering/trainA/Animation073.jpg  \n",
            "  inflating: rendering/trainA/Animation074.jpg  \n",
            "  inflating: rendering/trainA/Animation075.jpg  \n",
            "  inflating: rendering/trainA/Animation076.jpg  \n",
            "  inflating: rendering/trainA/Animation077.jpg  \n",
            "  inflating: rendering/trainA/Animation078.jpg  \n",
            "  inflating: rendering/trainA/Animation079.jpg  \n",
            "  inflating: rendering/trainA/Animation080.jpg  \n",
            "  inflating: rendering/trainA/Animation081.jpg  \n",
            "  inflating: rendering/trainA/Animation082.jpg  \n",
            "  inflating: rendering/trainA/Animation083.jpg  \n",
            "  inflating: rendering/trainA/Animation084.jpg  \n",
            "  inflating: rendering/trainA/Animation085.jpg  \n",
            "  inflating: rendering/trainA/Animation086.jpg  \n",
            "  inflating: rendering/trainA/Animation087.jpg  \n",
            "  inflating: rendering/trainA/Animation088.jpg  \n",
            "  inflating: rendering/trainA/Animation089.jpg  \n",
            "  inflating: rendering/trainA/Animation090.jpg  \n",
            "  inflating: rendering/trainA/Animation091.jpg  \n",
            "  inflating: rendering/trainA/Animation092.jpg  \n",
            "  inflating: rendering/trainA/Animation093.jpg  \n",
            "  inflating: rendering/trainA/Animation094.jpg  \n",
            "  inflating: rendering/trainA/Animation095.jpg  \n",
            "  inflating: rendering/trainA/Animation096.jpg  \n",
            "  inflating: rendering/trainA/Animation097.jpg  \n",
            "  inflating: rendering/trainA/Animation098.jpg  \n",
            "  inflating: rendering/trainA/Animation099.jpg  \n",
            "   creating: rendering/trainB/\n",
            "  inflating: rendering/trainB/Animation000.jpg  \n",
            "  inflating: rendering/trainB/Animation001.jpg  \n",
            "  inflating: rendering/trainB/Animation002.jpg  \n",
            "  inflating: rendering/trainB/Animation003.jpg  \n",
            "  inflating: rendering/trainB/Animation004.jpg  \n",
            "  inflating: rendering/trainB/Animation005.jpg  \n",
            "  inflating: rendering/trainB/Animation006.jpg  \n",
            "  inflating: rendering/trainB/Animation007.jpg  \n",
            "  inflating: rendering/trainB/Animation008.jpg  \n",
            "  inflating: rendering/trainB/Animation009.jpg  \n",
            "  inflating: rendering/trainB/Animation010.jpg  \n",
            "  inflating: rendering/trainB/Animation011.jpg  \n",
            "  inflating: rendering/trainB/Animation012.jpg  \n",
            "  inflating: rendering/trainB/Animation013.jpg  \n",
            "  inflating: rendering/trainB/Animation014.jpg  \n",
            "  inflating: rendering/trainB/Animation015.jpg  \n",
            "  inflating: rendering/trainB/Animation016.jpg  \n",
            "  inflating: rendering/trainB/Animation017.jpg  \n",
            "  inflating: rendering/trainB/Animation018.jpg  \n",
            "  inflating: rendering/trainB/Animation019.jpg  \n",
            "  inflating: rendering/trainB/Animation020.jpg  \n",
            "  inflating: rendering/trainB/Animation021.jpg  \n",
            "  inflating: rendering/trainB/Animation022.jpg  \n",
            "  inflating: rendering/trainB/Animation023.jpg  \n",
            "  inflating: rendering/trainB/Animation024.jpg  \n",
            "  inflating: rendering/trainB/Animation025.jpg  \n",
            "  inflating: rendering/trainB/Animation026.jpg  \n",
            "  inflating: rendering/trainB/Animation027.jpg  \n",
            "  inflating: rendering/trainB/Animation028.jpg  \n",
            "  inflating: rendering/trainB/Animation029.jpg  \n",
            "  inflating: rendering/trainB/Animation030.jpg  \n",
            "  inflating: rendering/trainB/Animation031.jpg  \n",
            "  inflating: rendering/trainB/Animation032.jpg  \n",
            "  inflating: rendering/trainB/Animation033.jpg  \n",
            "  inflating: rendering/trainB/Animation034.jpg  \n",
            "  inflating: rendering/trainB/Animation035.jpg  \n",
            "  inflating: rendering/trainB/Animation036.jpg  \n",
            "  inflating: rendering/trainB/Animation037.jpg  \n",
            "  inflating: rendering/trainB/Animation038.jpg  \n",
            "  inflating: rendering/trainB/Animation039.jpg  \n",
            "  inflating: rendering/trainB/Animation040.jpg  \n",
            "  inflating: rendering/trainB/Animation041.jpg  \n",
            "  inflating: rendering/trainB/Animation042.jpg  \n",
            "  inflating: rendering/trainB/Animation043.jpg  \n",
            "  inflating: rendering/trainB/Animation044.jpg  \n",
            "  inflating: rendering/trainB/Animation045.jpg  \n",
            "  inflating: rendering/trainB/Animation046.jpg  \n",
            "  inflating: rendering/trainB/Animation047.jpg  \n",
            "  inflating: rendering/trainB/Animation048.jpg  \n",
            "  inflating: rendering/trainB/Animation049.jpg  \n",
            "  inflating: rendering/trainB/Animation050.jpg  \n",
            "  inflating: rendering/trainB/Animation051.jpg  \n",
            "  inflating: rendering/trainB/Animation052.jpg  \n",
            "  inflating: rendering/trainB/Animation053.jpg  \n",
            "  inflating: rendering/trainB/Animation054.jpg  \n",
            "  inflating: rendering/trainB/Animation055.jpg  \n",
            "  inflating: rendering/trainB/Animation056.jpg  \n",
            "  inflating: rendering/trainB/Animation057.jpg  \n",
            "  inflating: rendering/trainB/Animation058.jpg  \n",
            "  inflating: rendering/trainB/Animation059.jpg  \n",
            "  inflating: rendering/trainB/Animation060.jpg  \n",
            "  inflating: rendering/trainB/Animation061.jpg  \n",
            "  inflating: rendering/trainB/Animation062.jpg  \n",
            "  inflating: rendering/trainB/Animation063.jpg  \n",
            "  inflating: rendering/trainB/Animation064.jpg  \n",
            "  inflating: rendering/trainB/Animation065.jpg  \n",
            "  inflating: rendering/trainB/Animation066.jpg  \n",
            "  inflating: rendering/trainB/Animation067.jpg  \n",
            "  inflating: rendering/trainB/Animation068.jpg  \n",
            "  inflating: rendering/trainB/Animation069.jpg  \n",
            "  inflating: rendering/trainB/Animation070.jpg  \n",
            "  inflating: rendering/trainB/Animation071.jpg  \n",
            "  inflating: rendering/trainB/Animation072.jpg  \n",
            "  inflating: rendering/trainB/Animation073.jpg  \n",
            "  inflating: rendering/trainB/Animation074.jpg  \n",
            "  inflating: rendering/trainB/Animation075.jpg  \n",
            "  inflating: rendering/trainB/Animation076.jpg  \n",
            "  inflating: rendering/trainB/Animation077.jpg  \n",
            "  inflating: rendering/trainB/Animation078.jpg  \n",
            "  inflating: rendering/trainB/Animation079.jpg  \n",
            "  inflating: rendering/trainB/Animation080.jpg  \n",
            "  inflating: rendering/trainB/Animation081.jpg  \n",
            "  inflating: rendering/trainB/Animation082.jpg  \n",
            "  inflating: rendering/trainB/Animation083.jpg  \n",
            "  inflating: rendering/trainB/Animation084.jpg  \n",
            "  inflating: rendering/trainB/Animation085.jpg  \n",
            "  inflating: rendering/trainB/Animation086.jpg  \n",
            "  inflating: rendering/trainB/Animation087.jpg  \n",
            "  inflating: rendering/trainB/Animation088.jpg  \n",
            "  inflating: rendering/trainB/Animation089.jpg  \n",
            "  inflating: rendering/trainB/Animation090.jpg  \n",
            "  inflating: rendering/trainB/Animation091.jpg  \n",
            "  inflating: rendering/trainB/Animation092.jpg  \n",
            "  inflating: rendering/trainB/Animation093.jpg  \n",
            "  inflating: rendering/trainB/Animation094.jpg  \n",
            "  inflating: rendering/trainB/Animation095.jpg  \n",
            "  inflating: rendering/trainB/Animation096.jpg  \n",
            "  inflating: rendering/trainB/Animation097.jpg  \n",
            "  inflating: rendering/trainB/Animation098.jpg  \n",
            "  inflating: rendering/trainB/Animation099.jpg  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gdUz4116xhpm"
      },
      "source": [
        "# Pretrained models\n",
        "\n",
        "Download one of the official pretrained models with:\n",
        "\n",
        "-   `bash ./scripts/download_cyclegan_model.sh [apple2orange, orange2apple, summer2winter_yosemite, winter2summer_yosemite, horse2zebra, zebra2horse, monet2photo, style_monet, style_cezanne, style_ukiyoe, style_vangogh, sat2map, map2sat, cityscapes_photo2label, cityscapes_label2photo, facades_photo2label, facades_label2photo, iphone2dslr_flower]`\n",
        "\n",
        "Or add your own pretrained model to `./checkpoints/{NAME}_pretrained/latest_net_G.pt`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B75UqtKhxznS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d42b14db-3bc7-44dc-fcac-16eb6a2838fe"
      },
      "source": [
        "!bash ./scripts/download_cyclegan_model.sh horse2zebra"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Note: available models are apple2orange, orange2apple, summer2winter_yosemite, winter2summer_yosemite, horse2zebra, zebra2horse, monet2photo, style_monet, style_cezanne, style_ukiyoe, style_vangogh, sat2map, map2sat, cityscapes_photo2label, cityscapes_label2photo, facades_photo2label, facades_label2photo, iphone2dslr_flower\n",
            "Specified [horse2zebra]\n",
            "WARNING: timestamping does nothing in combination with -O. See the manual\n",
            "for details.\n",
            "\n",
            "--2020-11-26 17:22:54--  http://efrosgans.eecs.berkeley.edu/cyclegan/pretrained_models/horse2zebra.pth\n",
            "Resolving efrosgans.eecs.berkeley.edu (efrosgans.eecs.berkeley.edu)... 128.32.189.73\n",
            "Connecting to efrosgans.eecs.berkeley.edu (efrosgans.eecs.berkeley.edu)|128.32.189.73|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 45575747 (43M)\n",
            "Saving to: ‘./checkpoints/horse2zebra_pretrained/latest_net_G.pth’\n",
            "\n",
            "./checkpoints/horse 100%[===================>]  43.46M  28.1MB/s    in 1.5s    \n",
            "\n",
            "2020-11-26 17:22:56 (28.1 MB/s) - ‘./checkpoints/horse2zebra_pretrained/latest_net_G.pth’ saved [45575747/45575747]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yFw1kDQBx3LN"
      },
      "source": [
        "# Training\n",
        "\n",
        "-   `python train.py --dataroot ./datasets/horse2zebra --name horse2zebra --model cycle_gan`\n",
        "\n",
        "Change the `--dataroot` and `--name` to your own dataset's path and model's name. Use `--gpu_ids 0,1,..` to train on multiple GPUs and `--batch_size` to change the batch size. I've found that a batch size of 16 fits onto 4 V100s and can finish training an epoch in ~90s.\n",
        "\n",
        "Once your model has trained, copy over the last checkpoint to a format that the testing model can automatically detect:\n",
        "\n",
        "Use `cp ./checkpoints/horse2zebra/latest_net_G_A.pth ./checkpoints/horse2zebra/latest_net_G.pth` if you want to transform images from class A to class B and `cp ./checkpoints/horse2zebra/latest_net_G_B.pth ./checkpoints/horse2zebra/latest_net_G.pth` if you want to transform images from class B to class A.\n",
        "\n",
        "**PHH** *You will need to create a new directory named* `checkpoints_2` *before proceeding.*\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0sp7TCT2x9dB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c9c093c8-21ff-4317-ab47-4c0c78153c16"
      },
      "source": [
        "!python train.py --dataroot ./datasets/rendering --name horse2zebra --model cycle_gan --checkpoints_dir checkpoints_2"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "----------------- Options ---------------\n",
            "               batch_size: 1                             \n",
            "                    beta1: 0.5                           \n",
            "          checkpoints_dir: checkpoints_2                 \t[default: ./checkpoints]\n",
            "           continue_train: False                         \n",
            "                crop_size: 256                           \n",
            "                 dataroot: ./datasets/rendering          \t[default: None]\n",
            "             dataset_mode: unaligned                     \n",
            "                direction: AtoB                          \n",
            "              display_env: main                          \n",
            "             display_freq: 400                           \n",
            "               display_id: 1                             \n",
            "            display_ncols: 4                             \n",
            "             display_port: 8097                          \n",
            "           display_server: http://localhost              \n",
            "          display_winsize: 256                           \n",
            "                    epoch: latest                        \n",
            "              epoch_count: 1                             \n",
            "                 gan_mode: lsgan                         \n",
            "                  gpu_ids: 0                             \n",
            "                init_gain: 0.02                          \n",
            "                init_type: normal                        \n",
            "                 input_nc: 3                             \n",
            "                  isTrain: True                          \t[default: None]\n",
            "                 lambda_A: 10.0                          \n",
            "                 lambda_B: 10.0                          \n",
            "          lambda_identity: 0.5                           \n",
            "                load_iter: 0                             \t[default: 0]\n",
            "                load_size: 286                           \n",
            "                       lr: 0.0002                        \n",
            "           lr_decay_iters: 50                            \n",
            "                lr_policy: linear                        \n",
            "         max_dataset_size: inf                           \n",
            "                    model: cycle_gan                     \n",
            "                 n_epochs: 100                           \n",
            "           n_epochs_decay: 100                           \n",
            "               n_layers_D: 3                             \n",
            "                     name: horse2zebra                   \t[default: experiment_name]\n",
            "                      ndf: 64                            \n",
            "                     netD: basic                         \n",
            "                     netG: resnet_9blocks                \n",
            "                      ngf: 64                            \n",
            "               no_dropout: True                          \n",
            "                  no_flip: False                         \n",
            "                  no_html: False                         \n",
            "                     norm: instance                      \n",
            "              num_threads: 4                             \n",
            "                output_nc: 3                             \n",
            "                    phase: train                         \n",
            "                pool_size: 50                            \n",
            "               preprocess: resize_and_crop               \n",
            "               print_freq: 100                           \n",
            "             save_by_iter: False                         \n",
            "          save_epoch_freq: 5                             \n",
            "         save_latest_freq: 5000                          \n",
            "           serial_batches: False                         \n",
            "                   suffix:                               \n",
            "         update_html_freq: 1000                          \n",
            "                  verbose: False                         \n",
            "----------------- End -------------------\n",
            "dataset [UnalignedDataset] was created\n",
            "The number of training images = 100\n",
            "initialize network with normal\n",
            "initialize network with normal\n",
            "initialize network with normal\n",
            "initialize network with normal\n",
            "model [CycleGANModel] was created\n",
            "---------- Networks initialized -------------\n",
            "[Network G_A] Total number of parameters : 11.378 M\n",
            "[Network G_B] Total number of parameters : 11.378 M\n",
            "[Network D_A] Total number of parameters : 2.765 M\n",
            "[Network D_B] Total number of parameters : 2.765 M\n",
            "-----------------------------------------------\n",
            "Setting up a new session...\n",
            "create web directory checkpoints_2/horse2zebra/web...\n",
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
            "learning rate 0.0002000 -> 0.0002000\n",
            "(epoch: 1, iters: 100, time: 0.507, data: 0.231) D_A: 0.245 G_A: 0.290 cycle_A: 1.073 idt_A: 0.201 D_B: 0.262 G_B: 0.238 cycle_B: 0.440 idt_B: 0.509 \n",
            "End of epoch 1 / 200 \t Time Taken: 47 sec\n",
            "learning rate 0.0002000 -> 0.0002000\n",
            "(epoch: 2, iters: 100, time: 0.519, data: 0.232) D_A: 0.354 G_A: 0.266 cycle_A: 0.861 idt_A: 0.163 D_B: 0.251 G_B: 0.264 cycle_B: 0.335 idt_B: 0.374 \n",
            "End of epoch 2 / 200 \t Time Taken: 48 sec\n",
            "learning rate 0.0002000 -> 0.0002000\n",
            "(epoch: 3, iters: 100, time: 0.530, data: 0.225) D_A: 0.239 G_A: 0.278 cycle_A: 0.752 idt_A: 0.109 D_B: 0.262 G_B: 0.253 cycle_B: 0.224 idt_B: 0.337 \n",
            "End of epoch 3 / 200 \t Time Taken: 49 sec\n",
            "learning rate 0.0002000 -> 0.0002000\n",
            "(epoch: 4, iters: 100, time: 0.776, data: 0.221) D_A: 0.538 G_A: 0.513 cycle_A: 0.673 idt_A: 0.130 D_B: 0.229 G_B: 0.279 cycle_B: 0.291 idt_B: 0.291 \n",
            "End of epoch 4 / 200 \t Time Taken: 50 sec\n",
            "learning rate 0.0002000 -> 0.0002000\n",
            "(epoch: 5, iters: 100, time: 0.540, data: 0.211) D_A: 0.263 G_A: 0.336 cycle_A: 0.642 idt_A: 0.108 D_B: 0.249 G_B: 0.272 cycle_B: 0.235 idt_B: 0.292 \n",
            "saving the model at the end of epoch 5, iters 500\n",
            "End of epoch 5 / 200 \t Time Taken: 51 sec\n",
            "learning rate 0.0002000 -> 0.0002000\n",
            "(epoch: 6, iters: 100, time: 0.542, data: 0.177) D_A: 0.254 G_A: 0.304 cycle_A: 0.560 idt_A: 0.098 D_B: 0.247 G_B: 0.269 cycle_B: 0.228 idt_B: 0.243 \n",
            "End of epoch 6 / 200 \t Time Taken: 51 sec\n",
            "learning rate 0.0002000 -> 0.0002000\n",
            "Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7f067da78dd8>>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1203, in __del__\n",
            "    self._shutdown_workers()\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1177, in _shutdown_workers\n",
            "    w.join(timeout=_utils.MP_STATUS_CHECK_INTERVAL)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 124, in join\n",
            "    res = self._popen.wait(timeout)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/popen_fork.py\", line 47, in wait\n",
            "    if not wait([self.sentinel], timeout):\n",
            "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 911, in wait\n",
            "    ready = selector.select(timeout)\n",
            "  File \"/usr/lib/python3.6/selectors.py\", line 376, in select\n",
            "    fd_event_list = self._poll.poll(timeout)\n",
            "KeyboardInterrupt: \n",
            "Traceback (most recent call last):\n",
            "  File \"train.py\", line 52, in <module>\n",
            "    model.optimize_parameters()   # calculate loss functions, get gradients, update network weights\n",
            "  File \"/content/pytorch-CycleGAN-and-pix2pix/models/cycle_gan_model.py\", line 188, in optimize_parameters\n",
            "    self.optimizer_G.step()       # update G_A and G_B's weights\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py\", line 67, in wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/autograd/grad_mode.py\", line 26, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/optim/adam.py\", line 119, in step\n",
            "    group['eps']\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/optim/functional.py\", line 98, in adam\n",
            "    param.addcdiv_(exp_avg, denom, value=-step_size)\n",
            "KeyboardInterrupt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9UkcaFZiyASl"
      },
      "source": [
        "# Testing\n",
        "\n",
        "-   `python test.py --dataroot datasets/horse2zebra/testA --name horse2zebra_pretrained --model test --no_dropout`\n",
        "\n",
        "Change the `--dataroot` and `--name` to be consistent with your trained model's configuration.\n",
        "\n",
        "> from https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix:\n",
        "> The option --model test is used for generating results of CycleGAN only for one side. This option will automatically set --dataset_mode single, which only loads the images from one set. On the contrary, using --model cycle_gan requires loading and generating results in both directions, which is sometimes unnecessary. The results will be saved at ./results/. Use --results_dir {directory_path_to_save_result} to specify the results directory.\n",
        "\n",
        "> For your own experiments, you might want to specify --netG, --norm, --no_dropout to match the generator architecture of the trained model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lODmvRDEhoVy"
      },
      "source": [
        "cp ./checkpoints/horse2zebra/latest_net_G_A.pth ./checkpoints/horse2zebra/latest_net_G.pth"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uCsKkEq0yGh0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "098d9abc-60c1-4e39-9b24-9f2116193722"
      },
      "source": [
        "!python test.py --dataroot datasets/rendering/testA --name horse2zebra --model test --no_dropout --checkpoints_dir checkpoints"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "----------------- Options ---------------\n",
            "             aspect_ratio: 1.0                           \n",
            "               batch_size: 1                             \n",
            "          checkpoints_dir: checkpoints                   \t[default: ./checkpoints]\n",
            "                crop_size: 256                           \n",
            "                 dataroot: datasets/rendering/testA      \t[default: None]\n",
            "             dataset_mode: single                        \n",
            "                direction: AtoB                          \n",
            "          display_winsize: 256                           \n",
            "                    epoch: latest                        \n",
            "                     eval: False                         \n",
            "                  gpu_ids: 0                             \n",
            "                init_gain: 0.02                          \n",
            "                init_type: normal                        \n",
            "                 input_nc: 3                             \n",
            "                  isTrain: False                         \t[default: None]\n",
            "                load_iter: 0                             \t[default: 0]\n",
            "                load_size: 256                           \n",
            "         max_dataset_size: inf                           \n",
            "                    model: test                          \n",
            "             model_suffix:                               \n",
            "               n_layers_D: 3                             \n",
            "                     name: horse2zebra                   \t[default: experiment_name]\n",
            "                      ndf: 64                            \n",
            "                     netD: basic                         \n",
            "                     netG: resnet_9blocks                \n",
            "                      ngf: 64                            \n",
            "               no_dropout: True                          \t[default: False]\n",
            "                  no_flip: False                         \n",
            "                     norm: instance                      \n",
            "                 num_test: 50                            \n",
            "              num_threads: 4                             \n",
            "                output_nc: 3                             \n",
            "                    phase: test                          \n",
            "               preprocess: resize_and_crop               \n",
            "              results_dir: ./results/                    \n",
            "           serial_batches: False                         \n",
            "                   suffix:                               \n",
            "                  verbose: False                         \n",
            "----------------- End -------------------\n",
            "dataset [SingleDataset] was created\n",
            "initialize network with normal\n",
            "model [TestModel] was created\n",
            "loading the model from checkpoints/horse2zebra/latest_net_G.pth\n",
            "---------- Networks initialized -------------\n",
            "[Network G] Total number of parameters : 11.378 M\n",
            "-----------------------------------------------\n",
            "creating web directory ./results/horse2zebra/test_latest\n",
            "processing (0000)-th image... ['datasets/rendering/testA/Animation000.jpg']\n",
            "processing (0005)-th image... ['datasets/rendering/testA/Animation005.jpg']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OzSKIPUByfiN"
      },
      "source": [
        "# Visualize"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Mgg8raPyizq"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "img = plt.imread('./results/horse2zebra_pretrained/test_latest/images/n02381460_1010_fake.png')\n",
        "plt.imshow(img)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0G3oVH9DyqLQ"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "img = plt.imread('./results/horse2zebra_pretrained/test_latest/images/n02381460_1010_real.png')\n",
        "plt.imshow(img)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j7v-7ajSbslY"
      },
      "source": [
        "# Download"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XFWKNhdKZfF8"
      },
      "source": [
        "**PHH:** *Zip `'results'` directory to be downloaded:*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gyj0yHcmvy-O",
        "outputId": "b779538e-0f0b-4014-c863-fdf6f0d99680"
      },
      "source": [
        "!zip -r ./results.zip ./results/"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "updating: results/ (stored 0%)\n",
            "updating: results/horse2zebra_pretrained/ (stored 0%)\n",
            "updating: results/horse2zebra_pretrained/test_latest/ (stored 0%)\n",
            "updating: results/horse2zebra_pretrained/test_latest/index.html (deflated 93%)\n",
            "updating: results/horse2zebra_pretrained/test_latest/images/ (stored 0%)\n",
            "updating: results/horse2zebra/ (stored 0%)\n",
            "updating: results/horse2zebra/test_latest/ (stored 0%)\n",
            "updating: results/horse2zebra/test_latest/index.html (deflated 93%)\n",
            "updating: results/horse2zebra/test_latest/images/ (stored 0%)\n",
            "updating: results/horse2zebra/test_latest/images/Animation002_real.png (deflated 4%)\n",
            "updating: results/horse2zebra/test_latest/images/Animation001_real.png (deflated 4%)\n",
            "updating: results/horse2zebra/test_latest/images/Animation001_fake.png (deflated 0%)\n",
            "updating: results/horse2zebra/test_latest/images/Animation005_real.png (deflated 5%)\n",
            "updating: results/horse2zebra/test_latest/images/Animation002_fake.png (deflated 0%)\n",
            "updating: results/horse2zebra/test_latest/images/Animation003_real.png (deflated 5%)\n",
            "updating: results/horse2zebra/test_latest/images/Animation007_real.png (deflated 4%)\n",
            "updating: results/horse2zebra/test_latest/images/Animation007_fake.png (deflated 0%)\n",
            "updating: results/horse2zebra/test_latest/images/Animation000_fake.png (deflated 0%)\n",
            "updating: results/horse2zebra/test_latest/images/Animation009_real.png (deflated 4%)\n",
            "updating: results/horse2zebra/test_latest/images/Animation005_fake.png (deflated 0%)\n",
            "updating: results/horse2zebra/test_latest/images/Animation000_real.png (deflated 5%)\n",
            "updating: results/horse2zebra/test_latest/images/Animation009_fake.png (deflated 0%)\n",
            "updating: results/horse2zebra/test_latest/images/Animation006_fake.png (deflated 0%)\n",
            "updating: results/horse2zebra/test_latest/images/Animation004_fake.png (deflated 0%)\n",
            "updating: results/horse2zebra/test_latest/images/Animation006_real.png (deflated 4%)\n",
            "updating: results/horse2zebra/test_latest/images/Animation008_real.png (deflated 4%)\n",
            "updating: results/horse2zebra/test_latest/images/Animation008_fake.png (deflated 0%)\n",
            "updating: results/horse2zebra/test_latest/images/Animation004_real.png (deflated 4%)\n",
            "updating: results/horse2zebra/test_latest/images/Animation003_fake.png (deflated 0%)\n",
            "updating: results/horse2zebra_pretrained/test_latest/images/.ipynb_checkpoints/ (stored 0%)\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}